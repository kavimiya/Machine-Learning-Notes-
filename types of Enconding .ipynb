{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce37005-3186-4126-8213-ebda57f08ede",
   "metadata": {},
   "source": [
    "# An Overview of Encoding Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143d4e96-da25-4cf4-adf3-eeb1b88efb81",
   "metadata": {},
   "source": [
    "# 1. Customer Churn Prediction (One-Hot Encoding)\n",
    "## Scenario:\n",
    "* A company wants to predict if a customer will churn based on their demographic details and interaction history. The dataset contains both numerical and categorical features (e.g., city, product type, and payment method).\n",
    "\n",
    "## Why Use One-Hot Encoding:\n",
    "* The categorical features (e.g., city or product type) are nominal (i.e., they have no inherent ranking). One-Hot Encoding works well here to convert these categories into a format suitable for algorithms like Logistic Regression, Decision Trees, and Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e82f041e-6b58-4098-87e7-b9d29a4a198e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       City Product  Churn  City_London  City_New York  City_Paris  \\\n",
      "0  New York   Phone      0        False           True       False   \n",
      "1    London  Tablet      1         True          False       False   \n",
      "2     Paris  Laptop      0        False          False        True   \n",
      "3    London   Phone      1         True          False       False   \n",
      "4  New York  Tablet      0        False           True       False   \n",
      "\n",
      "   Product_Laptop  Product_Phone  Product_Tablet  \n",
      "0           False           True           False  \n",
      "1           False          False            True  \n",
      "2            True          False           False  \n",
      "3           False           True           False  \n",
      "4           False          False            True  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example data for customer churn prediction\n",
    "data = {'City': ['New York', 'London', 'Paris', 'London', 'New York'],\n",
    "        'Product': ['Phone', 'Tablet', 'Laptop', 'Phone', 'Tablet'],\n",
    "        'Churn': [0, 1, 0, 1, 0]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply One-Hot Encoding to City and Product columns\n",
    "one_hot_encoded = pd.get_dummies(df[['City', 'Product']])\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the original DataFrame\n",
    "df = pd.concat([df, one_hot_encoded], axis=1)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0502923-3b0d-4beb-a416-c3095972384b",
   "metadata": {},
   "source": [
    "## When to Use:\n",
    "* Use One-Hot Encoding when your categorical features are nominal (no ranking).\n",
    "* This works well with algorithms like Logistic Regression, SVM, Neural Networks, and Tree-based models where categories are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6b3036-cf71-498a-bc85-c50a7e75132b",
   "metadata": {},
   "source": [
    "# 2. House Price Prediction (Label Encoding)\n",
    "## Scenario:\n",
    "* In a housing price prediction model, you have features like the condition of the house (Excellent, Good, Fair, Poor), which are ordinal in nature.\n",
    "\n",
    "## Why Use Label Encoding:\n",
    "* Since the house condition is ordinal (i.e., Excellent > Good > Fair > Poor), we can assign these categories numeric values that reflect their ranking. Label Encoding is appropriate when dealing with ordinal features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df57da18-be55-4ddd-b51d-444eba1eb8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Condition   Price  Condition_encoded\n",
      "0  Excellent  400000                  0\n",
      "1       Good  350000                  2\n",
      "2       Fair  300000                  1\n",
      "3       Poor  250000                  3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example data for house price prediction\n",
    "data = {'Condition': ['Excellent', 'Good', 'Fair', 'Poor'],\n",
    "        'Price': [400000, 350000, 300000, 250000]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize and apply Label Encoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['Condition_encoded'] = label_encoder.fit_transform(df['Condition'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfd70a1-b3b5-4f1f-8f50-ec3b04a48826",
   "metadata": {},
   "source": [
    "## When to Use:\n",
    "* Use Label Encoding for ordinal categories (those with a meaningful order) such as education levels, ratings (e.g., low, medium, high), and more.\n",
    "* It works with most algorithms but is especially useful in Decision Trees, Random Forests, and Gradient Boosting algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c54269-0f3f-4830-809b-2bb5742751f7",
   "metadata": {},
   "source": [
    "# 3. Fraud Detection (Frequency Encoding)\n",
    "## Scenario:\n",
    "* You are building a fraud detection model, and one of the features is the merchant where a transaction occurred. Some merchants appear frequently (common stores), while others are rare.\n",
    "\n",
    "## Why Use Frequency Encoding:\n",
    "* In this scenario, the frequency of a merchant can be an important indicator of fraud risk. Merchants with many transactions may have different risk profiles compared to merchants with very few transactions. Frequency Encoding replaces categories with how often they occur in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fc285d9-8b40-4262-813a-f7e7a39c07d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Merchant  Amount  Merchant_encoded\n",
      "0  Store_A     100                 2\n",
      "1  Store_B     200                 2\n",
      "2  Store_A     150                 2\n",
      "3  Store_C     400                 2\n",
      "4  Store_B     300                 2\n",
      "5  Store_C     250                 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example data for fraud detection\n",
    "data = {'Merchant': ['Store_A', 'Store_B', 'Store_A', 'Store_C', 'Store_B', 'Store_C'],\n",
    "        'Amount': [100, 200, 150, 400, 300, 250]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Frequency encoding for the 'Merchant' column\n",
    "merchant_frequency = df['Merchant'].value_counts().to_dict()\n",
    "df['Merchant_encoded'] = df['Merchant'].map(merchant_frequency)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632eb1d2-8f33-49db-86a4-250c1ec54aae",
   "metadata": {},
   "source": [
    "## When to Use:\n",
    "* Use Frequency Encoding when you believe the frequency of a category is relevant to the target outcome. This can be useful in fraud detection, customer segmentation, and recommendation systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bced935-3d66-41fc-acdb-69ddf699e9b4",
   "metadata": {},
   "source": [
    "# 4. Insurance Premium Prediction (Target Encoding)\n",
    "## Scenario:\n",
    "* You are predicting insurance premiums, and one feature is the type of vehicle (e.g., SUV, sedan, truck). Premium amounts may vary significantly based on the type of vehicle.\n",
    "\n",
    "## Why Use Target Encoding:\n",
    "* Target Encoding is beneficial when the categorical variable is highly predictive of the target. In this case, the type of vehicle directly impacts the premium, and we can use the average premium for each type of vehicle as the encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a47be1b-bf97-429c-9b59-5e0e8f54a8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Vehicle_Type  Premium  Vehicle_encoded\n",
      "0          SUV     1200           1225.0\n",
      "1        Sedan     1000           1050.0\n",
      "2        Truck      800            800.0\n",
      "3        Sedan     1100           1050.0\n",
      "4          SUV     1250           1225.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example data for insurance premium prediction\n",
    "data = {'Vehicle_Type': ['SUV', 'Sedan', 'Truck', 'Sedan', 'SUV'],\n",
    "        'Premium': [1200, 1000, 800, 1100, 1250]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate mean premium for each vehicle type (Target Encoding)\n",
    "mean_premium = df.groupby('Vehicle_Type')['Premium'].mean()\n",
    "df['Vehicle_encoded'] = df['Vehicle_Type'].map(mean_premium)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1a8310-49b9-41e1-ba94-ed12979aa443",
   "metadata": {},
   "source": [
    "## When to Use:\n",
    "* Use Target Encoding when the categorical variable has a strong relationship with the target (dependent variable). Be careful to avoid overfitting, especially on small datasets. It’s often used in regression tasks or when working with algorithms that can benefit from target leakage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a3d444-caa5-4425-b8f1-567becfabed5",
   "metadata": {},
   "source": [
    "# 5. Customer Segmentation (Binary Encoding)\n",
    "## Scenario:\n",
    "* You are segmenting customers based on different product categories (e.g., Electronics, Furniture, Apparel, etc.). The number of product categories is large, and you want to avoid creating too many features.\n",
    "\n",
    "## Why Use Binary Encoding:\n",
    "* Binary Encoding is useful when you have high-cardinality categorical features (i.e., many unique categories). Instead of creating many columns (as with One-Hot Encoding), Binary Encoding converts the category into a binary format, which is more memory-efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9d486b0-bc09-42d2-a447-edd41d89a226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Product_Category_0  Product_Category_1\n",
      "0                   0                   1\n",
      "1                   1                   0\n",
      "2                   1                   1\n",
      "3                   0                   1\n",
      "4                   1                   1\n"
     ]
    }
   ],
   "source": [
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "\n",
    "# Example data for customer segmentation\n",
    "data = {'Product_Category': ['Electronics', 'Furniture', 'Apparel', 'Electronics', 'Apparel']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply Binary Encoding\n",
    "binary_encoder = ce.BinaryEncoder(cols=['Product_Category'])\n",
    "df_encoded = binary_encoder.fit_transform(df)\n",
    "\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72388ae3-74c4-470c-a4e1-11610a1b1d53",
   "metadata": {},
   "source": [
    "## When to Use:\n",
    "* Use Binary Encoding when the categorical feature has many unique categories and you want to keep the feature space smaller.\n",
    "It’s a good choice for models like Tree-based algorithms (Random Forest, Gradient Boosting), or even Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4f5547-47e6-4305-a46f-cd12826487d0",
   "metadata": {},
   "source": [
    "# 6. Text Classification (Hashing Encoding)\n",
    "## Scenario:\n",
    "* You are building a spam email classifier where one of the features is the domain of the email sender. Since there can be thousands of unique email domains, using One-Hot Encoding would result in an unmanageable number of features.\n",
    "\n",
    "## Why Use Hashing Encoding:\n",
    "* For extremely high-cardinality features like email domains, Hashing Encoding is a fast and memory-efficient way to convert categories into numerical values without expanding the feature space excessively. This is especially helpful for real-time applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41793853-e5e0-49b6-a5b3-4936b2af26f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "# Example data for text classification (email domains)\n",
    "data = [{'Domain': 'gmail.com'}, {'Domain': 'yahoo.com'}, {'Domain': 'hotmail.com'}]\n",
    "\n",
    "# Hashing Encoding\n",
    "hasher = FeatureHasher(input_type='dict', n_features=5)\n",
    "hashed_features = hasher.transform(data).toarray()\n",
    "\n",
    "print(hashed_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d89b2-3722-45ac-b442-e3849575c5b2",
   "metadata": {},
   "source": [
    "## When to Use:\n",
    "* Use Hashing Encoding when dealing with very high-cardinality categorical features like URLs, email domains, user IDs, etc.\n",
    "Works well with algorithms like Logistic Regression, SVM, and Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe81a2a-862d-4578-8424-e924f1472366",
   "metadata": {},
   "source": [
    "## How to Choose the Right Encoding Technique:\n",
    "* One-Hot Encoding: Use when your categorical variable has no natural order (nominal) and there are a small number of unique categories.\n",
    "\n",
    "* Label Encoding: Use for ordinal variables with a clear ranking/order (education levels, ratings).\n",
    "\n",
    "* Frequency Encoding: Use when the frequency of a category carries meaning, as in fraud detection or sales prediction.\n",
    "\n",
    "* Target Encoding: Use when there is a strong relationship between the categorical variable and the target variable. Be cautious about overfitting.\n",
    "\n",
    "* Binary Encoding: Use for high-cardinality categorical features when you want a compact encoding method that works efficiently.\n",
    "\n",
    "* Hashing Encoding: Use when dealing with extremely high-cardinality features where the number of unique categories is very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ead63c-c7a2-4c39-b28c-8f2f10801901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
